---
flink:
  streamTimeCharacteristic: "ProcessingTime" ## ProcessingTime|IngestionTime|EventTime
  job:
    groupId: "${spring.application.name}-group"
    name: "${spring.application.name}"
    parallelism: "1"

kafka:
  bootstrapServers: "localhost:9092"
  zookeeperConnect: "localhost:2181"
  events:                                 ## Event will have a main source stream which will be split into
                                            #multiple substreams based on the below configuration of substream
                                            #list per source.
    sources:
      - topic: "log-event-stream-input"   ## Structured log file events, might require schema or some way to annotate the
                                           # event to identify the type of the event as it is not Json.
        siddhiStreamName: "S3logInputStream"
        "streamType": "log"               ## can be log (Structured log),json for now, this
                                            # is used to identify the split function
        subStreamType: "single"
        substreams:                       ## NOTE: right now S3 log main stream will have only one
                                            #substream nothing else
          - streamName: "s3.access.log"   ## final Substream name will be logInputStream-s3.access.log
                                            # the type and the stream name will be same and this will correspond to the type in each data
            type: "s3.access.log"         ## this is used for matching input data event type
            transformer: "s3-access-log"
            fields:
              - s3log
        sink:
          topic: "json-event-stream-output"
          outputStreamName: "outputstream"
  rules:                                  # Rule will have a main source stream which will be split into
                                            # multiple substreams based on the below configuration of substream list per source.
    sources:                              ## Always will be a json data from any source
      - topic: "si-simple-rules"          ## Main topic/source name where rules control events are published
        controlStreamName: "simplerulesStream"
        mappingSourceDataStream : "${kafka.events.sources[0].siddhiStreamName}" ## NOTE: This has to match with one of the
                                                                                # kafka.events.sources.[x].siddhiStreamName
        subStreams:
          - streamName: "s3.access.log"   ## final rule Substream name will be logInputStream-s3-access-log, this is needed to
                                          # map the rule substream with the data substream which catter to common types.
                                          # the type and the stream name will be same and this will correspond to the type in each data
            type: "s3.access.log"         #NOT USED RIGHT NOW
            transformer: "rule-transformer"
spring:
  application:
    name: "SIFlinkJob"
...