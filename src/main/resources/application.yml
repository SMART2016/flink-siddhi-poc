---
flink:
  streamTimeCharacteristic: "ProcessingTime" ## ProcessingTime|IngestionTime|EventTime
  job:
    groupId: "${spring.application.name}-group"
    name: "${spring.application.name}"
    parallelism: "1"

kafka:
  bootstrapServers: "localhost:9092"
  zookeeperConnect: "localhost:2181"
  events:                                 ## Event will have a main source stream which will be split into
    #multiple substreams based on the below configuration of substream
    #list per source.
    sources:
      - topic: "log-event-stream-input"   ## Structured log file events, might require schema or some way to annotate the
        # event to identify the type of the event as it is not Json.
        siddhiStreamName: "S3logInputStream"
        "streamType": "log"               ## can be log (Structured log),json for now, this
        # is used to identify the split function
        subStreamType: "single"
        substreams:                       ## NOTE: right now S3 log main stream will have only one
          #substream nothing else
          - type: "s3_access_log"         ## this is used for matching input data event type
            transformer: "s3-access-log-record"
            fields:
              - s3log
        sink:
          topic: "json-event-stream-output"
          outputStreamName: "outputstream"

      - topic: "json-event-stream-input"   ## json events
        # event to identify the type of the event as it is not Json.
        siddhiStreamName: "jsoninputstream"
        "streamType": "json"               ## can be log (Structured log),json for now, this
        # is used to identify the split function
        subStreamType: "multiple"
        substreams:                       ## NOTE: right now S3 log main stream will have only one
          #substream nothing else
          - type: "aws_s3"         ## this is used for matching input data event type
            transformer: "json-transformer"
            fields:
              - awsS3
        sink:
          topic: "s3-event-stream-output"
          outputStreamName: "s3_outputstream"
  rules:                                  # Rule will have a main source stream which will be split into
    # multiple substreams based on the below configuration of substream list per source.
    sources:                              ## Always will be a json data from any source
      - topic: "si-simple-rules"          ## Main topic/source name where rules control events are published
        controlStreamName: "simplerulesStream"
        mappingSourceDataStream: "${kafka.events.sources[0].siddhiStreamName}" ## NOTE: This has to match with one of the
        # kafka.events.sources.[x].siddhiStreamName
        subStreams:
          - type: "s3_access_log"
            transformer: "rule-transformer"
            #transformer: "rule-event-transformer"
          - type: "aws_s3"         ## this is used for matching input data event type
            transformer: "rule-transformer"

spring:
  application:
    name: "SIFlinkJob"
...